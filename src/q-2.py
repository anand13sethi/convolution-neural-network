#!/usr/bin/env python
# coding: utf-8

# # Question 2

# ### Part 1
#     - 156 parameters in 1st convolution layer.
# 
# ### Part 2
#     - There are no parameters on pooling layer.
#     
# ### Part 3
#     - Fully Connected layer has most number of paramters. (approx. 4800)
# 
# ### Part 4
#     - Fully connected layers take most amount of memory.
#
# ### Part 5:
# 
#     Relu had good performance but if we used tanh or simoid activation function the output is mostly of a block of single color with small points of other colour in between because the any value that is bit far away from 0 end up equal to 1 or -1/0.
# 




